---
jupyter:
  jupytext:
    text_representation:
      extension: .Rmd
      format_name: rmarkdown
      format_version: '1.2'
      jupytext_version: 1.16.1
  kernelspec:
    display_name: Python 3 (ipykernel)
    language: python
    name: python3
---

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
# Probability
<!-- #endregion -->

<!-- #region slideshow={"slide_type": ""} editable=true -->
>#### "One sees, from this Essay, that the theory of probabilities is basically just common sense reduced to calculus; it makes one appreciate with exactness that which accurate minds feel with a sort of instinct, often without being able to account for it."
> "Théorie Analytique des Probabilités" Pierre-Simon Laplace
<!-- #endregion -->

<!-- #region tags=["description"] editable=true slideshow={"slide_type": "skip"} -->
Because they deal with uncertain events, most of the machine learning methods can be framed in the language of probability. 
In this notebook I will very briefly recall the basics concepts of the probability calculus and introduce the notation I will be using, hopefully consistently, throughout the lecture.
<!-- #endregion -->

<!-- #region tags=["description"] editable=true slideshow={"slide_type": "skip"} -->
But keep in mind that this is not a supposed to be a textbook  on probability! Please treat this as a list of concepts and definitions that you have to refresh. It will also serve as a brief introduction to various Python packages. But again this is not a tutorial on Python. The code is provided as a guidance for you and it's up to you to lookup  explanantion in documentation if  needed. I  am of course also happy to help. You can consult me on the chat on Teams. 
<!-- #endregion -->

<!-- #region tags=["description"] editable=true slideshow={"slide_type": "skip"} -->
The lecture includes some simple problems to help you check your understanding of the subject. Some problems have answers right in the notebook. I will try to hide the content of this cells, please try to solve the problem before looking at the answer. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Random events
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
### Sample space
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Imagine any process that can have an upredictable outcome. This could be the results of a coin toss,  number of passengers on the bus etc. Let's  assume that we know the set of all possible outcomes of this process and call this set $S$.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$S$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
 This set is often called _sample space_.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Any subset $A$ of $S$ denoted
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$A\subseteq S$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
will be called an _event_. If process has an outcome $s\in S$ then we say that the event $A$ happened if $s\in A$. An event that contain only one  element 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$\{s\}$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
will be called an _elementary_ event, _atomic_ event or _sample point_.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Typical textbook example would be a  coin toss. In this case $ S=\{H, T\}$ and there are only four possible events (including the empty set).  There are two elementary events $\{H\}$ nad $\{T\}$.  
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
#### Example: Dice roll
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} -->
What is the sets of all possible outcomes of a roll of two dice? How many elements it contains? Write down the event $A$ - "the sum of the points is 9".  
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$S=\{(i,j): i,j\in\{1,2,3,4,5,6\}\},\quad \#S=36$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$A=\{(3,6), (4,5), (5,4), (6,3)\}\quad \#A = 4$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Where $\#A$ denotes the number of elements in set $A$.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
For larger examples this would be impractical, but just for fun let's code this in Python
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'slide'}}
from itertools import product
```

```{python editable=TRUE, slideshow={'slide_type': ''}}
S_dice =  {(i,j) for i,j in product(range(1,7), range(1,7))}
```

```{python}
print(len(S_dice))
print(S_dice)
```

```{python}
A = set( filter(lambda s: sum(s)==9, S_dice) )
print(A)
```

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Probability of an event
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Because the outcome of a process is unpredictable, so are the events.    However some events are more likely to happen then the others and we can quantify this by assigning  a number to each event that we call _probability_ of that event:
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$0\leq P(A) \leq 1$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
What this number really means is still subject to discussion and interpretation and I will not address this issue. Personaly I suport the Bayesian interpretation where probability is a measure of "degree of certainty" with zero probability denoting _impossible_ event and one denoting a _certain_ event.  What is important is that those numbers cannot be totaly arbitrary. To be considered a valid measure, probabilities must satisfy several  axioms consistent with our common sense: 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
1. Probability is non-negative 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(A)\ge 0$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
2. Probability event $S$ is one as one of the possible outcomes _must_ happen
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(S)=1$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
3. Probability of a sum of disjoint events is the sum of the probabilities of each event.</br>
    For any integer $k>1$ including $k=\infty$ if events $A_i$ are mutually disjoint that is for each $i\neq j$ $A_i \cap A_j =\varnothing$ then 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(A_1\cup A_2\cup\cdots \cup A_k) = P(A_1)+P(A_2) + \cdots + P(A_k)\qquad i\neq j\; A_i \cap A_j =\varnothing$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
    An important colorary is that when the set of outcomes is countable the probability of an event $A$ is the sum of the probabilities for each elementary event contained in $A$:
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A) = \sum_{s\in A}P(\{s\})$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
A set is countable when we can assign an unique natural number to each of its elements, in other word we can count its elements. All finite sets are of course countable. An example of not countable set is provided e.g. by the real numbers or any interval $[a,b)$ with $b>a$. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
It follows from 3. that in case of countable outcomes it is enough to specify the probability of each elementary event, as they are clearly disjoint and each event can be  represented a a sum of elementary events. 

In the following  I will ommit the set parenthesis for the elementary events i.e. assume 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(s)\equiv P(\{s\}).$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
From axiom 2.  we have 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$\sum_{s\in S} P(s) = 1$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
#### Problem: Complementary event
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Prove that 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(S\setminus A)= 1-P(A)\text{ where } S\setminus A = \{s\in S: s\notin A\}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
__Answer__
<!-- #endregion -->

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
It follows directly from the second and third axiom after noting that
<!-- #endregion -->

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "slide"} jupyter={"source_hidden": true} -->
$$(S\setminus A) \cup A = S \text{ and } (S\setminus A) \cap A = \varnothing$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Law of large numbers
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
The connection between $P(A)$ and "reality" is made by the [law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). Losely speaking it states that if we observe our process many times, the frequency  of occurence of an event $A$ will be proportional to the probability $P(A)$. This is the fundation of the frequentionist interpretation of probability. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Calculating probability
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
The concept of the probability can be somewhat hazy and verges upon philosophy. My take on this is that to calculate the probability we need a _model_ of the process. E.g. for the dice example the model is that all elementary events are equally probable, leading to assignement of probability $1/36$ to every possible two dice roll outcome. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
The connection with experiment (reality) is given by the [Borel's law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). It states that if we repeat an experiment under same conditions many times, the fraction of times an event happens will converge to the probability of this event. This is a fundation of _frequentist_ interpretation of probability. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
It is harder to interpret the probability of one-off events _e.g._ "there is a 30% chance that it will rain tomorrow", or "there is 80% chance that Real Madrid will win La Liga this year" in view of the frequentist interpretation. However we can still use the Bayesian "degree of certainty(belief)" interpretation in this case. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Conditional probability
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
How does a probability of an event change when we know that some other event happed? That is a central question in machine learning and is  answered by _conditional_ probability
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A|B)$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
This denotes the probability that event $A$ happened on  condition that the event also $B$ happend. The formal definition is
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A|B) = \frac{P(A\cap B)}{P(B)}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
From this defintion it follows  that
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A\cap B)=P(A|B) P(B)$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
This is called [_product or chain rule_](https://en.wikipedia.org/wiki/Chain_rule_(probability)) and is very useful for specyfying the probability. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
#### Example
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} -->
Let's take as an example roll of two dice. What is the probability that  the sum is six ?
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'fragment'}}
A = set( filter(lambda s: sum(s)==6, S_dice) )
print(len(A),  A)
```

```{python editable=TRUE, slideshow={'slide_type': 'slide'}}
# Just to have nice fractions instead of floats
from fractions import Fraction 
P_A =  Fraction(len(A),len(S_dice))
print(P_A, float(P_A))
```

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
And now suppose that someone tells us that we have rolled three on one die. Did the the probability change?  
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Again I will use some Python code althought it is probably faster to   calculate this "by hand". Try it before proceding further.
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'fragment'}}
B = set( filter(lambda s: s[0]==3 or s[1]==3 , S_dice) )
print(len(B), B)
```

```{python editable=TRUE, slideshow={'slide_type': 'fragment'}}
P_B = Fraction(len(B), len(S_dice))
print(P_B)
```

```{python editable=TRUE, slideshow={'slide_type': 'slide'}}
A_cap_B = A.intersection(B)
P_A_cap_B = Fraction(len(A_cap_B), len(S_dice))
print(len(A_cap_B), A_cap_B)
```

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
And finally
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'fragment'}}
P_A_cond_B = P_A_cap_B/P_B
print(P_A_cond_B, float(P_A_cond_B))
```

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
So this is smaller  then $P(A)=5/36$. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
#### Problem
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": ""} -->
__1.__ What if we are told that we have rolled one on one die? Has the conditional probability of rolling six changed? 
<!-- #endregion -->

```{python tags=c("remove", "skip"), editable=TRUE, slideshow={'slide_type': 'skip'}}
B1 = set( filter(lambda s: s[0]==1 or s[1]==1 , S_dice) )
A_cap_B1 = A.intersection(B1)
```

```{python tags=c("remove", "skip"), editable=TRUE, slideshow={'slide_type': 'skip'}}
Fraction(len(A_cap_B1), len(B1))
```

<!-- #region editable=true slideshow={"slide_type": ""} -->
__2.__ Calculate the conditional probability 
$$P(d_1 + d_2 = 6\, |\, d_1 = i \vee d_2 = i)\quad\text{for}\;\; i=1,\ldots,6$$
where $d_1$ and $d_2$ are the outcomes of fisrt and second dice roll respetcively. 
<!-- #endregion -->

```{python tags=c("remove", "skip"), editable=TRUE, slideshow={'slide_type': 'skip'}}
Bi = [set( filter(lambda s: s[0]==i or s[1]==i , S_dice) ) for i in range(1,7)]
A_cap_Bi = [A.intersection(s) for s in Bi]
```

```{python tags=c("remove", "skip"), editable=TRUE, slideshow={'slide_type': 'skip'}}
[Fraction(len(n),len(d)) for n,d in zip(A_cap_Bi, Bi)]
```

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
The product rule 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(A\cap B)=P(A|B) P(B)$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
provides a sometimes convenient way of calculating probabilities. If we assume that set $B$ can be partitioned into disjoined sets $B_i$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$B=\bigcup_i B_i,\quad B_i\cap B_j=\varnothing\;\;i\neq j$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
then 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A\cap B)=\sum_i P(A\cap B_i)=\sum_i P(A|B_i) P(B_i)$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
as the sets $A\cap B_i$ are also mutually disjoined. In case when $B$ is the whole sampling space
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$S=\bigcup_i B_i,\quad B_i\cap B_j=\varnothing\;\;i\neq j$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
the probability $P(A)$ can be rewritten as a sum of conditional probabilities
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A)=\sum_i P(A|B_i) P(B_i)$$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
which are sometimes easier to calculate or estimate. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Bayes theorem
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
It is very important to keep in mind that conditional probability $P(A|B)$ is not symetric! _E.g._ when it rains the probability that sidewalk will be wet is one. On the other hand when the sidewalk is wet it does not imply  with certainty that it has rained, it may have  been _e.g._ washed by our neighbour. But as we will see many times in course of this lecture the ability to "invert" conditional probability comes in very handy. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
By definition
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(B|A) = \frac{P(A \cap B)}{P(A)}\quad\text{and}\quad P(A|B) = \frac{P(A \cap B)}{P(B)}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
we can use second expression to calculate $P(A\cap B)$ and subsitute it into first to obtain
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$\large\boxed{P(B|A) = \frac{P(A|B)P(B)}{P(A)}}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
This formula is know as Bayes theorem. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
#### Problem: Wet sidewalk
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Let's apply it to the "wet sidewalk problem". We look in the morning through our window and see wet sidewalk. What is the probability that it has rained at night? 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
#### Answer
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(rain|wet)= \frac{P(wet|rain)P(rain)}{P(wet)}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
If $wet$ is the event "sidewalk is wet" and $rain$ is the event "it has rained" then 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(wet|rain)=1$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
 and according to Bayes theorem
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(rain|wet)=\frac{P(rain)}{P(wet)}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Now we need to determine $P(wet)$. The sidewalk can get wet in to ways: either it rains or it does not rain and our neigbour washes the sidewalk so  
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(wet) = P(rain) + P(wash|\neg rain)P(\neg rain) = P(rain) + P(wash|\neg rain)(1-P(rain))$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
and
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(rain|wet) = \frac{P(rain)}{P(rain)+P(wash|\neg rain)(1-P(rain))}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Let's consider some "corner cases". If our neigbour always washes the sidewalk when it does not rain then the results is $P(rain)$ - sidewalk is always wet, we do not have any additional information.  
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
If our neigbour never washes the sidewalk then results is one - the only reason for wet sidewalk is rain so when it is wet it must have rained.
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
If our neighbour washed the sidewalk only half of the times when it does not rain we obtain
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
$$P(rain|wet) = \frac{P(rain)}{P(rain)+\frac{1}{2}(1-P(rain))} = \frac{ 2 P(rain)}{1+P(rain)}$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
So if _e.g._ $P(rain)=1/7$  seeing wet sidewalk increses that chance to
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'skip'}}
print(2 * Fraction(1,7)/(1+Fraction(1,7)))
```

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Let's plot this using `matplotlib`  and `numpy` libraries. 
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'skip'}}
import numpy as np
import matplotlib.pyplot as plt
# %matplotlib inline
plt.rcParams["figure.figsize"] = [12,8]
```

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
We can plot the whole family of plots corresponding to different values of $P(wash|\neg rain)$
<!-- #endregion -->

```{python tags=c("hide_src"), editable=TRUE, slideshow={'slide_type': 'skip'}}
ps = np.linspace(0,1,100)
plt.xlabel("P(rain)")
plt.ylabel("P(rain|wet)");
plt.plot(ps, ps, c='grey', linewidth = 1);
for pw in [0.1, 0.2, 0.3, 0.4, 0.5, 0.75]:
    plt.plot(ps, ps/(ps+pw*(1-ps)),label = "P(w|not r) = {:.2f}".format(pw)); 
plt.grid()
plt.legend();
```

<!-- #region slideshow={"slide_type": "slide", "slideshow": {"slide_type": "slide"}} tags=["problem"] editable=true -->
#### Problem: Base rate fallacy
<!-- #endregion -->

<!-- #region tags=["problem"] editable=true slideshow={"slide_type": ""} -->
You are tested for a rare disease (1 person in 250). Test has 80%  true positive rate and  10% false positive rate. i.e. test gives positive (you are ill) result for 80% of ill patients and for 10% of healthy patients.   

Your are tested positive, what are the chances you have the disease? 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
#### Answer
<!-- #endregion -->

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
What we need is the  probability that we are ill on condition that we have been tested positive:
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "slide"} tags=["answer"] editable=true jupyter={"source_hidden": true} -->
$$P(ill|P)= \frac{P(P|ill)P(ill)}{P(P)}$$
<!-- #endregion -->

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
The probability of being ill and tested positive is 
<!-- #endregion -->

```{python tags=c("answer"), editable=TRUE, slideshow={'slide_type': 'fragment'}, jupyter={'source_hidden': True}}
p_ill_cond_p = 0.8  
```

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
The probability of being tested positive is
<!-- #endregion -->

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "fragment"} jupyter={"source_hidden": true} -->
$$P(P)=P(P|ill)P(ill)+P(P|\neg ill)P(\neg ill)$$
<!-- #endregion -->

```{python editable=TRUE, slideshow={'slide_type': 'slide'}}
p_ill = 0.004
```

```{python tags=c("answer"), editable=TRUE, slideshow={'slide_type': 'fragment'}, jupyter={'source_hidden': True}}
p_p = 0.8*p_ill + 0.1*(1-p_ill)
```

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
and finally 
<!-- #endregion -->

```{python tags=c("answer"), editable=TRUE, slideshow={'slide_type': 'fragment'}, jupyter={'source_hidden': True}}
p_ill_cond_p = p_ill_cond_p*p_ill/p_p
print("{:4.1f}%".format(100*p_ill_cond_p))
```

<!-- #region tags=["answer"] editable=true slideshow={"slide_type": "skip"} jupyter={"source_hidden": true} -->
So there is no cause to despair yet :) 
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "slide", "slideshow": {"slide_type": "slide"}} editable=true -->
### Increase of information (learning)
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
One could say that this test is useless if positive  result gives only $3\%$ chance of being ill. And  this particular test was actually discarde but it is not totaly useless. Before taking the test our chance of being ill was $0.4\%$. After seing the positive result it "jumped" more then ten times to $3.1\%$. 
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "fragment"} editable=true -->
$$0.004 \longrightarrow 0.031$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
After seing a negative result our chances of being ill dropped four times:
<!-- #endregion -->

<!-- #region slideshow={"slide_type": "fragment"} editable=true -->
$$0.004 \longrightarrow 0.001 $$ 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
## Independent events
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
It may happen that  knowledge that $B$ happened does not change  the probability of $A$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "slide"} -->
$$P(A|B) = P(A)$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
We say then that  events $A$ and $B$ are _independent_. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
For example when tossing the coin the outcome of toss does not depend in any way on the outcome of previous tosses or in case of dice the  face they land on are independent etc. 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
Substituting the definition of conditional independence 
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$\frac{P(A\cap B)}{P(B)}  = P(A)$$
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "skip"} -->
we obtain  a more familiar factorisation criterion for joint probability of independent events
<!-- #endregion -->

<!-- #region editable=true slideshow={"slide_type": "fragment"} -->
$$P(A\cap B) = P(A)\cdot P(B)$$
<!-- #endregion -->
